{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c850f02e-2b30-4b07-b4f8-4736c30f67c4",
   "metadata": {},
   "source": [
    "## NLP ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97a1c2-1793-4b76-83d6-0f4fe3f216e3",
   "metadata": {},
   "source": [
    "## N-gram models ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0969c-3873-44ca-9336-12ee50be3f8f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is an n-gram language model?\n",
    "\n",
    "An **n-gram language model** is a **probabilistic model** that predicts the next word in a sequence by looking at the previous *n-1* words.\n",
    "\n",
    "* **n-gram** = a sequence of *n* consecutive words (or tokens) in text.\n",
    "* The idea:\n",
    "\n",
    "  $$\n",
    "  P(w_k \\mid w_1, w_2, \\dots, w_{k-1}) \\approx P(w_k \\mid w_{k-n+1}, \\dots, w_{k-1})\n",
    "  $$\n",
    "\n",
    "  ‚Üí instead of conditioning on the **entire history**, we only condition on the **last (n-1) words**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Examples\n",
    "\n",
    "Corpus:\n",
    "\n",
    "```\n",
    "I love machine learning\n",
    "I love deep learning\n",
    "```\n",
    "\n",
    "### 1-gram (Unigram)\n",
    "\n",
    "* Just single words: `I`, `love`, `machine`, `learning`, `deep`\n",
    "* Probabilities: $P(w) = \\frac{\\text{count}(w)}{\\text{total words}}$\n",
    "\n",
    "  * e.g., `P(love) = 2/8`\n",
    "\n",
    "### 2-gram (Bigram)\n",
    "\n",
    "* Pairs of words: `I love`, `love machine`, `machine learning`, `love deep`, `deep learning`\n",
    "* Probabilities:\n",
    "\n",
    "  $$\n",
    "  P(w_k \\mid w_{k-1}) = \\frac{\\text{count}(w_{k-1}, w_k)}{\\text{count}(w_{k-1})}\n",
    "  $$\n",
    "\n",
    "  * e.g., `P(machine | love) = count(\"love machine\") / count(\"love\") = 1/2`\n",
    "\n",
    "### 3-gram (Trigram)\n",
    "\n",
    "* Triplets of words: `I love machine`, `love machine learning`, `I love deep`, `love deep learning`\n",
    "* Same formula but with 2-word history.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why use n-grams?\n",
    "\n",
    "‚úÖ Captures local context of words.\n",
    "‚úÖ Simple and fast to train.\n",
    "‚ö†Ô∏è But‚Ä¶\n",
    "\n",
    "* **Data sparsity** ‚Üí many n-grams never occur in training data.\n",
    "* **Memory explosion** ‚Üí large n means huge vocabulary size.\n",
    "* **Short context** ‚Üí doesn‚Äôt capture long-range dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example Usage\n",
    "\n",
    "Suppose we want to predict the next word after **‚ÄúI love‚Äù**:\n",
    "\n",
    "* From training:\n",
    "\n",
    "  * `P(machine | I love) = 0.5`\n",
    "  * `P(deep | I love) = 0.5`\n",
    "\n",
    "So the model predicts **either \"machine\" or \"deep\"** with equal probability.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Improvements\n",
    "\n",
    "* **Smoothing** (Laplace, Kneser-Ney) ‚Üí handle unseen n-grams.\n",
    "* **Backoff / Interpolation** ‚Üí combine smaller n-grams with larger ones.\n",
    "* **Neural models** (like RNNs, Transformers) ‚Üí overcome data sparsity and capture longer context.\n",
    "\n",
    "---\n",
    "\n",
    "üëâ Would you like me to also make a **small Python example** that builds a bigram model from a toy corpus and shows how it predicts the next word?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f784c5-8482-46f7-b5ff-aedfd8ed5bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0df4ce-20c8-4ca4-9b05-a3ee699eeb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a58ef9-f2cd-465e-addd-bfda01787fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
